[
  {
    "chunk_id": "EU_AI_Act_Doc_docx_0_832b6196",
    "text": "High-level summary of the AI Act 27 Feb, 2024 Updated on 30 May in accordance with the Corrigendum version of the AI Act. In this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text. To explore the full text of the AI Act yourself, use our AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our Compliance Checker. Four-point summary",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 0,
      "chunk_index": 0,
      "char_count": 594
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_1_8c7c283e",
    "text": "The AI Act classifies AI according to its risk: Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI). Most of the text addresses high-risk AI systems, which are regulated. A smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes). Minimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI). The majority of obligations fall on providers (developers) of high-risk AI systems. Those that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country. And also third country providers where the high risk AI system’s output is used in the EU. Users are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users. Users (deployers) of high-risk AI systems have some obligations, though less than providers (developers). This applies to users located in the EU, and third country users where the AI system’s output is used in the EU.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 1,
      "chunk_index": 1,
      "char_count": 1306
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_2_392fa055",
    "text": "General purpose AI (GPAI): All GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training. Free and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk. All providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections. Prohibited AI systems (Chapter II, Art. 5) The following types of AI system are ‘Prohibited’ according to the AI Act.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 2,
      "chunk_index": 2,
      "char_count": 687
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_3_1a3d511b",
    "text": "AI systems: deploying subliminal, manipulative, or deceptive techniques to distort behaviour and impair informed decision-making, causing significant harm. exploiting vulnerabilities related to age, disability, or socio-economic circumstances to distort behaviour, causing significant harm. biometric categorisation systems inferring sensitive attributes (race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation), except labelling or filtering of lawfully acquired biometric datasets or when law enforcement categorises biometric data. social scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people. assessing the risk of an individual committing criminal offenses solely based on profiling or personality traits, except when used to augment human assessments based on objective, verifiable facts directly linked to criminal activity. compiling facial recognition databases by untargeted scraping of facial images from the internet or CCTV footage. inferring emotions in workplaces or educational institutions, except for medical or safety reasons",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 3,
      "chunk_index": 3,
      "char_count": 1224
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_4_1d15c1f5",
    "text": ". inferring emotions in workplaces or educational institutions, except for medical or safety reasons. ‘real-time’ remote biometric identification (RBI) in publicly accessible spaces for law enforcement, except when: searching for missing persons, abduction victims, and people who have been human trafficked or sexually exploited; preventing substantial and imminent threat to life, or foreseeable terrorist attack; or identifying suspects in serious crimes (e.g., murder, rape, armed robbery, narcotic and illegal weapons trafficking, organised crime, and environmental crime, etc.).",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 3,
      "chunk_index": 4,
      "char_count": 584
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_5_90642b9c",
    "text": "Notes on remote biometric identification: Using AI-enabled real-time RBI is only allowed when not using the tool would cause considerable harm and must account for affected persons’ rights and freedoms. Before deployment, police must complete a fundamental rights impact assessment and register the system in the EU database, though, in duly justified cases of urgency, deployment can commence without registration, provided that it is registered later without undue delay. Before deployment, they also must obtain authorisation from a judicial authority or independent administrative authority[1], though, in duly justified cases of urgency, deployment can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs. ↲ [1] Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 2024). High risk AI systems (Chapter III) Some AI systems are considered ‘High risk’ under the AI Act. Providers of those systems will be subject to additional requirements. Classification rules for high-risk AI systems (Art. 6)",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 4,
      "chunk_index": 5,
      "char_count": 1208
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_6_954ef326",
    "text": "High risk AI systems are those: used as a safety component or a product covered by EU laws in Annex I AND required to undergo a third-party conformity assessment under those Annex I laws; OR",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 5,
      "chunk_index": 6,
      "char_count": 190
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_7_5f87c680",
    "text": "those under Annex III use cases (below), except if: the AI system performs a narrow procedural task; improves the result of a previously completed human activity; detects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or performs a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III. AI systems are always considered high-risk if it profiles individuals, i.e. automated processing of personal data to assess various aspects of a person’s life, such as work performance, economic situation, health, preferences, interests, reliability, behaviour, location or movement. Providers whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such an assessment before placing it on the market or putting it into service. Requirements for providers of high-risk AI systems (Art. 8–17)",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 6,
      "chunk_index": 7,
      "char_count": 1006
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_8_49e54abf",
    "text": "High risk AI providers must: Establish a risk management system throughout the high risk AI system’s lifecycle; Conduct data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose. Draw up technical documentation to demonstrate compliance and provide authorities with the information to assess that compliance. Design their high risk AI system for record-keeping to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle. Provide instructions for use to downstream deployers to enable the latter’s compliance. Design their high risk AI system to allow deployers to implement human oversight. Design their high risk AI system to achieve appropriate levels of accuracy, robustness, and cybersecurity. Establish a quality management system to ensure compliance. General purpose AI (GPAI) GPAI model means an AI model, including when trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable to competently perform a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 7,
      "chunk_index": 8,
      "char_count": 1387
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_9_ee2c7c4f",
    "text": ". This does not cover AI models that are used before release on the market for research, development and prototyping activities. GPAI system means an AI system which is based on a general purpose AI model, that has the capability to serve a variety of purposes, both for direct use as well as for integration in other AI systems. GPAI systems may be used as high risk AI systems or integrated into them. GPAI system providers should cooperate with such high risk AI system providers to enable the latter’s compliance.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 7,
      "chunk_index": 9,
      "char_count": 517
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_10_64a1f247",
    "text": "All providers of GPAI models must: Draw up technical documentation, including training and testing process and evaluation results. Draw up information and documentation to supply to downstream providers that intend to integrate the GPAI model into their own AI system in order that the latter understands capabilities and limitations and is enabled to comply. Establish a policy to respect the Copyright Directive. Publish a sufficiently detailed summary about the content used for training the GPAI model. Free and open licence GPAI models – whose parameters, including weights, model architecture and model usage are publicly available, allowing for access, usage, modification and distribution of the model – only have to comply with the latter two obligations above, unless the free and open licence GPAI model is systemic. GPAI models present systemic risks when the cumulative amount of compute used for its training is greater than 1025 floating point operations (FLOPs). Providers must notify the Commission if their model meets this criterion within 2 weeks. The provider may present arguments that, despite meeting the criteria, their model does not present systemic risks. The Commission may decide on its own, or via a qualified alert from the scientific panel of independent experts, that a model has high impact capabilities, rendering it systemic.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 8,
      "chunk_index": 10,
      "char_count": 1362
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_11_99a39508",
    "text": "In addition to the four obligations above, providers of GPAI models with systemic risk must also: Perform model evaluations, including conducting and documenting adversarial testing to identify and mitigate systemic risk. Assess and mitigate possible systemic risks, including their sources. Track, document and report serious incidents and possible corrective measures to the AI Office and relevant national competent authorities without undue delay. Ensure an adequate level of cybersecurity protection. All GPAI model providers may demonstrate compliance with their obligations if they voluntarily adhere to a code of practice until European harmonised standards are published, compliance with which will lead to a presumption of conformity. Providers that don’t adhere to codes of practice must demonstrate alternative adequate means of compliance for Commission approval. Codes of practice Will account for international approaches. Will cover but not necessarily limited to the above obligations, particularly the relevant information to include in technical documentation for authorities and downstream providers, identification of the type and nature of systemic risks and their sources, and the modalities of risk management accounting for specific challenges in addressing risks due to the way they may emerge and materialise throughout the value chain",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 9,
      "chunk_index": 11,
      "char_count": 1362
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_12_100ebb9d",
    "text": ". AI Office may invite GPAI model providers, relevant national competent authorities to participate in drawing up the codes, while civil society, industry, academia, downstream providers and independent experts may support the process. Governance How will the AI Act be implemented? The AI Office will be established, sitting within the Commission, to monitor the effective implementation and compliance of GPAI model providers. Downstream providers can lodge a complaint regarding the upstream providers infringement to the AI Office.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 9,
      "chunk_index": 12,
      "char_count": 535
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_13_c9c0c528",
    "text": "The AI Office may conduct evaluations of the GPAI model to: assess compliance where the information gathered under its powers to request information is insufficient. Investigate systemic risks, particularly following a qualified report from the scientific panel of independent experts. Timelines",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 10,
      "chunk_index": 13,
      "char_count": 295
    }
  },
  {
    "chunk_id": "EU_AI_Act_Doc_docx_14_e3696cc2",
    "text": "After entry into force, the AI Act will apply by the following deadlines: 6 months for prohibited AI systems. 12 months for GPAI. 24 months for high risk AI systems under Annex III. 36 months for high risk AI systems under Annex I.",
    "metadata": {
      "source": "EU AI Act Doc.docx",
      "doc_name": "eu_ai_act",
      "doc_type": "docx",
      "section": 11,
      "chunk_index": 14,
      "char_count": 231
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_0_e804bc6e",
    "text": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 1,
      "chunk_index": 0,
      "char_count": 1781
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_1_18a30ca0",
    "text": ". ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 1,
      "chunk_index": 1,
      "char_count": 1116
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_2_b1c93376",
    "text": "1 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 2,
      "chunk_index": 2,
      "char_count": 1730
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_3_dc336b7d",
    "text": ". In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 2,
      "chunk_index": 3,
      "char_count": 1620
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_4_825cebda",
    "text": ". End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 2,
      "chunk_index": 4,
      "char_count": 1102
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_5_16742d07",
    "text": "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 3,
      "chunk_index": 5,
      "char_count": 1782
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_6_6a826b6e",
    "text": ". 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 3,
      "chunk_index": 6,
      "char_count": 218
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_7_202faead",
    "text": "Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention( Q, K, V ) = softmax(QKT √dk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 4,
      "chunk_index": 7,
      "char_count": 1778
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_8_3d6a8cda",
    "text": ". We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by1√dk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and variance dk. 4",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 4,
      "chunk_index": 8,
      "char_count": 867
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_9_1c639a7f",
    "text": "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the projections are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel. In this work we employ h= 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: •In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. •The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. •Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 5,
      "chunk_index": 9,
      "char_count": 1700
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_10_773656be",
    "text": ". •Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN( x) = max(0 , xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel. 5",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 5,
      "chunk_index": 10,
      "char_count": 1630
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_11_16a8540c",
    "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(logk(n)) Self-Attention (restricted) O(r·n·d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: PE(pos,2i)=sin(pos/100002i/d model) PE(pos,2i+1)=cos(pos/100002i/d model) where posis the position and iis the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E))",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 6,
      "chunk_index": 11,
      "char_count": 1759
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_12_fe5cb834",
    "text": ". We also experimented with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 6,
      "chunk_index": 12,
      "char_count": 1740
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_13_59dc980e",
    "text": ". As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 6,
      "chunk_index": 13,
      "char_count": 303
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_14_e494a426",
    "text": "length nis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [ 31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 7,
      "chunk_index": 14,
      "char_count": 1719
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_15_8a92b694",
    "text": ". 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning rate over the course of training, according to the formula: lrate =d−0.5 model·min(step_num−0.5, step _num·warmup _steps−1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We employ three types of regularization during training: 7",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 7,
      "chunk_index": 15,
      "char_count": 1656
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_16_dd0b12dd",
    "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0·1020 GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020 ConvS2S [9] 25.16 40.46 9.6·10181.5·1020 MoE [32] 26.03 40.56 2.0·10191.2·1020 Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020 GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021 ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021 Transformer (base model) 27.3 38.1 3.3·1018 Transformer (big) 28.4 41.8 2.3·1019 Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop= 0.1. Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 8,
      "chunk_index": 16,
      "char_count": 1696
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_17_027c59f7",
    "text": ". Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 8,
      "chunk_index": 17,
      "char_count": 1636
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_18_6b4daa96",
    "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N d model dff h d k dvPdrop ϵlstrain PPL BLEU params steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 9,
      "chunk_index": 18,
      "char_count": 1677
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_19_5674df1a",
    "text": ". We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 9,
      "chunk_index": 19,
      "char_count": 1430
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_20_4cd15db2",
    "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 10,
      "chunk_index": 20,
      "char_count": 1792
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_21_570f3837",
    "text": ". On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor . Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. [2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR , abs/1409.0473, 2014. [3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural machine translation architectures. CoRR , abs/1703.03906, 2017. [4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 , 2016. 10",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 10,
      "chunk_index": 21,
      "char_count": 1439
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_22_11a35d8a",
    "text": "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR , abs/1406.1078, 2014. [6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357 , 2016. [7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014. [8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016. [9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 11,
      "chunk_index": 22,
      "char_count": 1795
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_23_97b08b51",
    "text": ". ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016. [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016. [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR) , 2016. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. InInternational Conference on Learning Representations , 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017. [23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015. 11",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 11,
      "chunk_index": 23,
      "char_count": 1578
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_24_0943c8c3",
    "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pages 152–159. ACL, June 2006. [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing , 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 , 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(1):1929–1958, 2014",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 12,
      "chunk_index": 24,
      "char_count": 1733
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_25_968bcdaa",
    "text": ". Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(1):1929–1958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates, Inc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434–443. ACL, August 2013. 12",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 12,
      "chunk_index": 25,
      "char_count": 1625
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_26_6919213c",
    "text": "Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 13,
      "chunk_index": 26,
      "char_count": 812
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_27_4a08ecca",
    "text": "Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 14,
      "chunk_index": 27,
      "char_count": 814
    }
  },
  {
    "chunk_id": "Attention_is_all_you_need_pdf_28_bcc615ed",
    "text": "Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15",
    "metadata": {
      "source": "Attention_is_all_you_need.pdf",
      "doc_name": "attention",
      "doc_type": "pdf",
      "page": 15,
      "chunk_index": 28,
      "char_count": 817
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_0_b2240d06",
    "text": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. AIME 2024 (Pass@1)Codeforces (Percentile)GPQA Diamond (Pass@1)MATH-500 (Pass@1)MMLU (Pass@1)SWE-bench Verified (Resolved)020406080100Accuracy / Percentile (%)79.896.3 71.597.3 90.8 49.279.296.6 75.796.4 91.8 48.972.690.6 62.194.3 87.4 36.863.693.4 60.090.0 85.2 41.6 39.258.7 59.190.2 88.5 42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1|Benchmark performance of DeepSeek-R1.arXiv:2501.12948v1 [cs.CL] 22 Jan 2025",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 1,
      "chunk_index": 0,
      "char_count": 1466
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_1_879b42da",
    "text": "Contents 1 Introduction 3 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Approach 5 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5 2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9 2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10 2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10 2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11 2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11 3 Experiment 11 3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4 Discussion 14 4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5 Conclusion, Limitations, and Future Work 16 A Contributions and Acknowledgments 20 2",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 2,
      "chunk_index": 1,
      "char_count": 1787
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_2_2ea42087",
    "text": "1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 3,
      "chunk_index": 2,
      "char_count": 1726
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_3_bb7c0f75",
    "text": ". Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. We further explore distillation from DeepSeek-R1 to smaller dense models",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 3,
      "chunk_index": 3,
      "char_count": 1778
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_4_675bbfd3",
    "text": ". We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. 3",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 3,
      "chunk_index": 4,
      "char_count": 626
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_5_eb078a50",
    "text": "1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model •We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. •We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too •We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. •Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 4,
      "chunk_index": 5,
      "char_count": 1734
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_6_8d1dac1d",
    "text": ". The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 1.2. Summary of Evaluation Results •Reasoning tasks : (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. •Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark. 4",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 4,
      "chunk_index": 6,
      "char_count": 1799
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_7_601349f9",
    "text": "•Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. 2. Approach 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 5,
      "chunk_index": 7,
      "char_count": 1616
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_8_02356fec",
    "text": ". However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data , focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1. Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1,𝑜2,···,𝑜𝐺}from the old policy𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective: J𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺 𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)] 1 𝐺𝐺∑︁ 𝑖=1 min𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖, clip𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞), 1−𝜀, 1+𝜀 𝐴𝑖 −𝛽D𝐾𝐿 𝜋𝜃||𝜋𝑟𝑒𝑓 ,(1) D𝐾𝐿 𝜋𝜃||𝜋𝑟𝑒𝑓=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞)−1, (2) where𝜀and𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of rewards{𝑟1,𝑟2,...,𝑟𝐺}corresponding to the outputs within each group: 𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺}) s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3) 5",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 5,
      "chunk_index": 8,
      "char_count": 1372
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_9_7579cf3f",
    "text": "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant: Table 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training. 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: •Accuracy rewards : The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. •Format rewards : In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. 2.2.3",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 6,
      "chunk_index": 9,
      "char_count": 1714
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_10_faaa46f6",
    "text": ". 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strate- gies—to ensure that we can accurately observe the model’s natural progression during the RL process. 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model’s performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers 6",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 6,
      "chunk_index": 10,
      "char_count": 1450
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_11_948f85ae",
    "text": "ModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843 DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444 Table 2|Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. Figure 2|AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model’s progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 7,
      "chunk_index": 11,
      "char_count": 1769
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_12_1d03fff7",
    "text": ". This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve- 7",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 7,
      "chunk_index": 12,
      "char_count": 236
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_13_d4d84d32",
    "text": "Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 8,
      "chunk_index": 13,
      "char_count": 1651
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_14_09cec4c2",
    "text": ". During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an “aha moment” for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The “aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. 8",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 8,
      "chunk_index": 14,
      "char_count": 874
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_15_b46cd324",
    "text": "Question: If 𝑎 >1, then the sum of the real solutions of√︁ 𝑎−√ 𝑎+𝑥=𝑥is equal to Response: <think> To solve the equation√︁ 𝑎−√ 𝑎+𝑥=𝑥, let’s start by squaring both ···√︁ 𝑎−√ 𝑎+𝑥2 =𝑥2=⇒𝑎−√ 𝑎+𝑥=𝑥2. Rearrange to isolate the inner square root term: (𝑎−𝑥2)2=𝑎+𝑥=⇒𝑎2−2𝑎𝑥2+(𝑥2)2=𝑎+𝑥=⇒𝑥4−2𝑎𝑥2−𝑥+(𝑎2−𝑎)=0 . . . Wait, wait. Wait. That’s an aha moment I can flag here. Let’s reevaluate this step-by-step to identify if the correct sum can be ··· We started with the equation:√︁ 𝑎−√ 𝑎+𝑥=𝑥 First, let’s square both sides: 𝑎−√ 𝑎+𝑥=𝑥2=⇒√ 𝑎+𝑥=𝑎−𝑥2 Next, I could square both sides again, treating the equation: ··· . . . Table 3|An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 9,
      "chunk_index": 15,
      "char_count": 1314
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_16_9fda973a",
    "text": ". To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. 2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data 9",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 9,
      "chunk_index": 16,
      "char_count": 1509
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_17_3864367c",
    "text": "include: •Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. •Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model’s performance, this reward aligns with human preferences, making it more readable",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 10,
      "chunk_index": 17,
      "char_count": 1771
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_18_50a66b9e",
    "text": ". Although ablation experiments show that such alignment results in a slight degradation in the model’s performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. 10",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 10,
      "chunk_index": 18,
      "char_count": 1711
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_19_5b496b56",
    "text": "Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as “hello” we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model’s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 11,
      "chunk_index": 19,
      "char_count": 1631
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_20_cbff571b",
    "text": ". For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. 3. Experiment Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 11",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 11,
      "chunk_index": 20,
      "char_count": 1787
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_21_e1869016",
    "text": "2024d), Aider1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP , and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 12,
      "chunk_index": 21,
      "char_count": 1616
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_22_b388d589",
    "text": ". Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@ 𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6and a top- 𝑝value of 0.95 to generate 𝑘 responses (typically between 4and 64, depending on the test set size) for each question. Pass@1 is then calculated as pass@1 =1 𝑘𝑘∑︁ 𝑖=1𝑝𝑖, where𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 12,
      "chunk_index": 22,
      "char_count": 1773
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_23_e6e09f6a",
    "text": ". This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 12",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 12,
      "chunk_index": 23,
      "char_count": 279
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_24_2da1ae38",
    "text": "3.1. DeepSeek-R1 Evaluation Benchmark (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 0513 V3 o1-mini o1-1217 R1 Architecture - - MoE - - MoE # Activated Params - - 37B - - 37B # Total Params - - 671B - - 671B EnglishMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9 MMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0 DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3 GPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5 AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6 ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3 CodeLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9 Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Codeforces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3 MathAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8 ChineseCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8 C-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8 C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7 Table 4|Comparison between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 13,
      "chunk_index": 24,
      "char_count": 1793
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_25_57fd695d",
    "text": ". Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model’s ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that 13",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 13,
      "chunk_index": 25,
      "char_count": 1518
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_26_4eab0f55",
    "text": "DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2. Distilled Model Evaluation ModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 Table 5|Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 14,
      "chunk_index": 26,
      "char_count": 1723
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_27_29398fec",
    "text": ". DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. 4. Discussion 4.1. Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale 14",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 14,
      "chunk_index": 27,
      "char_count": 1070
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_28_8b038abf",
    "text": "ModelAIME 2024 MATH-500 GPQA Diamond LiveCodeBench pass@1 cons@64 pass@1 pass@1 pass@1 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 Table 6|Comparison of distilled and RL Models on Reasoning-Related Benchmarks. RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning. 4.2. Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 15,
      "chunk_index": 28,
      "char_count": 1790
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_29_5a5d1991",
    "text": ". First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con- ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an 15",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 15,
      "chunk_index": 29,
      "char_count": 1747
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_30_976c6135",
    "text": "exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo’s core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation. In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge. 5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 16,
      "chunk_index": 30,
      "char_count": 1793
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_31_3cfe482d",
    "text": ". Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. •General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. •Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. •Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results. •Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. 16",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 16,
      "chunk_index": 31,
      "char_count": 1668
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_32_3561d19b",
    "text": "References AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md . Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet . M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet, F. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 . A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, B. Galambosi, P . Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179 . L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760 . A. P . Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 17,
      "chunk_index": 32,
      "char_count": 1799
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_33_c98e2095",
    "text": ". Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760 . A. P . Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P . Minervini. Are we done with mmlu? CoRR , abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127 . Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024 . Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974 . 17",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 17,
      "chunk_index": 33,
      "char_count": 1415
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_34_d3ac1471",
    "text": "S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR , abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941 . A. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212 , 2023. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. H. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023. B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval . MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination -AIME 2024 , February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime . OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/ . OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/ . OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/ . OpenAI",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 18,
      "chunk_index": 34,
      "char_count": 1733
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_35_9f36fc80",
    "text": ". Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/ . OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/ . OpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/ . Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm .github.io/blog/qwq-32b-preview/ . Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5 . D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 , 2023. Z. Shao, P . Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P . Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR , abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815 . 18",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 18,
      "chunk_index": 35,
      "char_count": 1303
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_36_6796fccf",
    "text": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P . Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat. , 550(7676):354–359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270 . C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14. T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. P . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935 , 2023. X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR , abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574 . C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 19,
      "chunk_index": 36,
      "char_count": 1796
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_37_2aee297c",
    "text": ". C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152 . J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 19",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 19,
      "chunk_index": 37,
      "char_count": 618
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_38_8cd2d063",
    "text": "Appendix A. Contributions and Acknowledgments Core Contributors Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z.F. Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao Contributors Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo* Guangbo Hao Guanting Chen Guowei Li H. Zhang Hanwei Xu Honghui Ding Huazuo Gao Hui QuHui Li Jianzhong Guo Jiashi Li Jingchang Chen Jingyang Yuan Jinhao Tu Junjie Qiu Junlong Li J.L. Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu* Kaichao You Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge* Ruisong Zhang Ruizhe Pan Runji Wang R.J. Chen R.L. Jin 20",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 20,
      "chunk_index": 38,
      "char_count": 1029
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_39_5f0c0324",
    "text": "Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S.S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu* Wentao Zhang W.L. Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X.Q. Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xinyi Zhou Xianzu Wang Xinxia Shan Y.K. Li Y.Q. WangY.X. Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma* Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y.X. Zhu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z.Z. Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang Yan Zhiyu Wu Zihui Gu 21",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 21,
      "chunk_index": 39,
      "char_count": 1080
    }
  },
  {
    "chunk_id": "Deepseek-r1_pdf_40_139c91c6",
    "text": "Zijia Zhu Zijun Liu* Zilin Li Ziwei Xie Ziyang Song Zizheng PanZhen Huang Zhipeng Xu Zhongyu Zhang Zhen Zhang Within each role, authors are listed alphabetically by the first name. Names marked with * denote individuals who have departed from our team. 22",
    "metadata": {
      "source": "Deepseek-r1.pdf",
      "doc_name": "deepseek",
      "doc_type": "pdf",
      "page": 22,
      "chunk_index": 40,
      "char_count": 255
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_0_fcae86f5",
    "text": "Sheet: INFLATION.CALCULATOR This is an inflation calculator with historical CPI data from 1913 to 2022. Available data columns: Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Average This data can be used to calculate inflation-adjusted values between any two years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "summary",
      "chunk_index": 0,
      "char_count": 285
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_1_067abb11",
    "text": "Inflation data for year 1913: Monthly CPI values: Jan: 9.8, Feb: 9.8, Mar: 9.8, Apr: 9.8, May: 9.7, Jun: 9.8, Jul: 9.9, Aug: 9.9, Sep: 10, Oct: 10, Nov: 10.1, Dec: 10 Average annual CPI for 1913: 9.883333333333335 Year 1913 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1913 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 14,
      "chunk_index": 1,
      "char_count": 374
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_2_1f7a1a77",
    "text": "Inflation data for year 1914: Monthly CPI values: Jan: 10, Feb: 9.9, Mar: 9.9, Apr: 9.8, May: 9.9, Jun: 9.9, Jul: 10, Aug: 10.2, Sep: 10.2, Oct: 10.1, Nov: 10.2, Dec: 10.1 Average annual CPI for 1914: 10.016666666666666 Year 1914 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1914 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 15,
      "chunk_index": 2,
      "char_count": 380
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_3_71a62096",
    "text": "Inflation data for year 1915: Monthly CPI values: Jan: 10.1, Feb: 10, Mar: 9.9, Apr: 10, May: 10.1, Jun: 10.1, Jul: 10.1, Aug: 10.1, Sep: 10.1, Oct: 10.2, Nov: 10.3, Dec: 10.3 Average annual CPI for 1915: 10.108333333333333 Year 1915 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1915 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 16,
      "chunk_index": 3,
      "char_count": 384
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_4_0b97f46f",
    "text": "Inflation data for year 1916: Monthly CPI values: Jan: 10.4, Feb: 10.4, Mar: 10.5, Apr: 10.6, May: 10.7, Jun: 10.8, Jul: 10.8, Aug: 10.9, Sep: 11.1, Oct: 11.3, Nov: 11.5, Dec: 11.6 Average annual CPI for 1916: 10.883333333333333 Year 1916 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1916 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 17,
      "chunk_index": 4,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_5_370209c7",
    "text": "Inflation data for year 1917: Monthly CPI values: Jan: 11.7, Feb: 12, Mar: 12, Apr: 12.6, May: 12.8, Jun: 13, Jul: 12.8, Aug: 13, Sep: 13.3, Oct: 13.5, Nov: 13.5, Dec: 13.7 Average annual CPI for 1917: 12.824999999999998 Year 1917 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1917 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 18,
      "chunk_index": 5,
      "char_count": 381
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_6_acb474b6",
    "text": "Inflation data for year 1918: Monthly CPI values: Jan: 14, Feb: 14.1, Mar: 14, Apr: 14.2, May: 14.5, Jun: 14.7, Jul: 15.1, Aug: 15.4, Sep: 15.7, Oct: 16, Nov: 16.3, Dec: 16.5 Average annual CPI for 1918: 15.041666666666666 Year 1918 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1918 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 19,
      "chunk_index": 6,
      "char_count": 383
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_7_85b2b295",
    "text": "Inflation data for year 1919: Monthly CPI values: Jan: 16.5, Feb: 16.2, Mar: 16.4, Apr: 16.7, May: 16.9, Jun: 16.9, Jul: 17.4, Aug: 17.7, Sep: 17.8, Oct: 18.1, Nov: 18.5, Dec: 18.9 Average annual CPI for 1919: 17.333333333333332 Year 1919 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1919 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 20,
      "chunk_index": 7,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_8_7b24c890",
    "text": "Inflation data for year 1920: Monthly CPI values: Jan: 19.3, Feb: 19.5, Mar: 19.7, Apr: 20.3, May: 20.6, Jun: 20.9, Jul: 20.8, Aug: 20.3, Sep: 20, Oct: 19.9, Nov: 19.8, Dec: 19.4 Average annual CPI for 1920: 20.04166666666667 Year 1920 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1920 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 21,
      "chunk_index": 8,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_9_21f14141",
    "text": "Inflation data for year 1921: Monthly CPI values: Jan: 19, Feb: 18.4, Mar: 18.3, Apr: 18.1, May: 17.7, Jun: 17.6, Jul: 17.7, Aug: 17.7, Sep: 17.5, Oct: 17.5, Nov: 17.4, Dec: 17.3 Average annual CPI for 1921: 17.850000000000005 Year 1921 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1921 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 22,
      "chunk_index": 9,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_10_0791ece7",
    "text": "Inflation data for year 1922: Monthly CPI values: Jan: 16.9, Feb: 16.9, Mar: 16.7, Apr: 16.7, May: 16.7, Jun: 16.7, Jul: 16.8, Aug: 16.6, Sep: 16.6, Oct: 16.7, Nov: 16.8, Dec: 16.9 Average annual CPI for 1922: 16.75 Year 1922 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1922 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 23,
      "chunk_index": 10,
      "char_count": 376
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_11_fb4aca3d",
    "text": "Inflation data for year 1923: Monthly CPI values: Jan: 16.8, Feb: 16.8, Mar: 16.8, Apr: 16.9, May: 16.9, Jun: 17, Jul: 17.2, Aug: 17.1, Sep: 17.2, Oct: 17.3, Nov: 17.3, Dec: 17.3 Average annual CPI for 1923: 17.050000000000004 Year 1923 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1923 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 24,
      "chunk_index": 11,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_12_44cedb29",
    "text": "Inflation data for year 1924: Monthly CPI values: Jan: 17.3, Feb: 17.2, Mar: 17.1, Apr: 17, May: 17, Jun: 17, Jul: 17.1, Aug: 17, Sep: 17.1, Oct: 17.2, Nov: 17.2, Dec: 17.3 Average annual CPI for 1924: 17.124999999999996 Year 1924 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1924 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 25,
      "chunk_index": 12,
      "char_count": 381
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_13_9b312ce0",
    "text": "Inflation data for year 1925: Monthly CPI values: Jan: 17.3, Feb: 17.2, Mar: 17.3, Apr: 17.2, May: 17.3, Jun: 17.5, Jul: 17.7, Aug: 17.7, Sep: 17.7, Oct: 17.7, Nov: 18, Dec: 17.9 Average annual CPI for 1925: 17.541666666666664 Year 1925 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1925 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 26,
      "chunk_index": 13,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_14_4e96fa43",
    "text": "Inflation data for year 1926: Monthly CPI values: Jan: 17.9, Feb: 17.9, Mar: 17.8, Apr: 17.9, May: 17.8, Jun: 17.7, Jul: 17.5, Aug: 17.4, Sep: 17.5, Oct: 17.6, Nov: 17.7, Dec: 17.7 Average annual CPI for 1926: 17.7 Year 1926 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1926 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 27,
      "chunk_index": 14,
      "char_count": 375
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_15_909b6bfe",
    "text": "Inflation data for year 1927: Monthly CPI values: Jan: 17.5, Feb: 17.4, Mar: 17.3, Apr: 17.3, May: 17.4, Jun: 17.6, Jul: 17.3, Aug: 17.2, Sep: 17.3, Oct: 17.4, Nov: 17.3, Dec: 17.3 Average annual CPI for 1927: 17.358333333333338 Year 1927 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1927 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 28,
      "chunk_index": 15,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_16_abc5d06e",
    "text": "Inflation data for year 1928: Monthly CPI values: Jan: 17.3, Feb: 17.1, Mar: 17.1, Apr: 17.1, May: 17.2, Jun: 17.1, Jul: 17.1, Aug: 17.1, Sep: 17.3, Oct: 17.2, Nov: 17.2, Dec: 17.1 Average annual CPI for 1928: 17.15833333333333 Year 1928 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1928 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 29,
      "chunk_index": 16,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_17_b8a7d5cc",
    "text": "Inflation data for year 1929: Monthly CPI values: Jan: 17.1, Feb: 17.1, Mar: 17, Apr: 16.9, May: 17, Jun: 17.1, Jul: 17.3, Aug: 17.3, Sep: 17.3, Oct: 17.3, Nov: 17.3, Dec: 17.2 Average annual CPI for 1929: 17.158333333333335 Year 1929 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1929 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 30,
      "chunk_index": 17,
      "char_count": 385
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_18_1e21c079",
    "text": "Inflation data for year 1930: Monthly CPI values: Jan: 17.1, Feb: 17, Mar: 16.9, Apr: 17, May: 16.9, Jun: 16.8, Jul: 16.6, Aug: 16.5, Sep: 16.6, Oct: 16.5, Nov: 16.4, Dec: 16.1 Average annual CPI for 1930: 16.7 Year 1930 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1930 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 31,
      "chunk_index": 18,
      "char_count": 371
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_19_e50e5a42",
    "text": "Inflation data for year 1931: Monthly CPI values: Jan: 15.9, Feb: 15.7, Mar: 15.6, Apr: 15.5, May: 15.3, Jun: 15.1, Jul: 15.1, Aug: 15.1, Sep: 15, Oct: 14.9, Nov: 14.7, Dec: 14.6 Average annual CPI for 1931: 15.20833333333333 Year 1931 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1931 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 32,
      "chunk_index": 19,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_20_be8ab881",
    "text": "Inflation data for year 1932: Monthly CPI values: Jan: 14.3, Feb: 14.1, Mar: 14, Apr: 13.9, May: 13.7, Jun: 13.6, Jul: 13.6, Aug: 13.5, Sep: 13.4, Oct: 13.3, Nov: 13.2, Dec: 13.1 Average annual CPI for 1932: 13.641666666666666 Year 1932 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1932 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 33,
      "chunk_index": 20,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_21_e0204c03",
    "text": "Inflation data for year 1933: Monthly CPI values: Jan: 12.9, Feb: 12.7, Mar: 12.6, Apr: 12.6, May: 12.6, Jun: 12.7, Jul: 13.1, Aug: 13.2, Sep: 13.2, Oct: 13.2, Nov: 13.2, Dec: 13.2 Average annual CPI for 1933: 12.933333333333332 Year 1933 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1933 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 34,
      "chunk_index": 21,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_22_5cc6184d",
    "text": "Inflation data for year 1934: Monthly CPI values: Jan: 13.2, Feb: 13.3, Mar: 13.3, Apr: 13.3, May: 13.3, Jun: 13.4, Jul: 13.4, Aug: 13.4, Sep: 13.6, Oct: 13.5, Nov: 13.5, Dec: 13.4 Average annual CPI for 1934: 13.383333333333333 Year 1934 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1934 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 35,
      "chunk_index": 22,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_23_65b37712",
    "text": "Inflation data for year 1935: Monthly CPI values: Jan: 13.6, Feb: 13.7, Mar: 13.7, Apr: 13.8, May: 13.8, Jun: 13.7, Jul: 13.7, Aug: 13.7, Sep: 13.7, Oct: 13.7, Nov: 13.8, Dec: 13.8 Average annual CPI for 1935: 13.725000000000001 Year 1935 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1935 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 36,
      "chunk_index": 23,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_24_cc4b2279",
    "text": "Inflation data for year 1936: Monthly CPI values: Jan: 13.8, Feb: 13.8, Mar: 13.7, Apr: 13.7, May: 13.7, Jun: 13.8, Jul: 13.9, Aug: 14, Sep: 14, Oct: 14, Nov: 14, Dec: 14 Average annual CPI for 1936: 13.866666666666667 Year 1936 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1936 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 37,
      "chunk_index": 24,
      "char_count": 379
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_25_eb7670dd",
    "text": "Inflation data for year 1937: Monthly CPI values: Jan: 14.1, Feb: 14.1, Mar: 14.2, Apr: 14.3, May: 14.4, Jun: 14.4, Jul: 14.5, Aug: 14.5, Sep: 14.6, Oct: 14.6, Nov: 14.5, Dec: 14.4 Average annual CPI for 1937: 14.383333333333335 Year 1937 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1937 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 38,
      "chunk_index": 25,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_26_c51ec8e2",
    "text": "Inflation data for year 1938: Monthly CPI values: Jan: 14.2, Feb: 14.1, Mar: 14.1, Apr: 14.2, May: 14.1, Jun: 14.1, Jul: 14.1, Aug: 14.1, Sep: 14.1, Oct: 14, Nov: 14, Dec: 14 Average annual CPI for 1938: 14.091666666666663 Year 1938 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1938 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 39,
      "chunk_index": 26,
      "char_count": 383
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_27_18f3f4c4",
    "text": "Inflation data for year 1939: Monthly CPI values: Jan: 14, Feb: 13.9, Mar: 13.9, Apr: 13.8, May: 13.8, Jun: 13.8, Jul: 13.8, Aug: 13.8, Sep: 14.1, Oct: 14, Nov: 14, Dec: 14 Average annual CPI for 1939: 13.908333333333331 Year 1939 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1939 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 40,
      "chunk_index": 27,
      "char_count": 381
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_28_7ccbb6f6",
    "text": "Inflation data for year 1940: Monthly CPI values: Jan: 13.9, Feb: 14, Mar: 14, Apr: 14, May: 14, Jun: 14.1, Jul: 14, Aug: 14, Sep: 14, Oct: 14, Nov: 14, Dec: 14.1 Average annual CPI for 1940: 14.008333333333333 Year 1940 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1940 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 41,
      "chunk_index": 28,
      "char_count": 371
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_29_25b99b04",
    "text": "Inflation data for year 1941: Monthly CPI values: Jan: 14.1, Feb: 14.1, Mar: 14.2, Apr: 14.3, May: 14.4, Jun: 14.7, Jul: 14.7, Aug: 14.9, Sep: 15.1, Oct: 15.3, Nov: 15.4, Dec: 15.5 Average annual CPI for 1941: 14.725000000000003 Year 1941 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1941 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 42,
      "chunk_index": 29,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_30_bd6ddead",
    "text": "Inflation data for year 1942: Monthly CPI values: Jan: 15.7, Feb: 15.8, Mar: 16, Apr: 16.1, May: 16.3, Jun: 16.3, Jul: 16.4, Aug: 16.5, Sep: 16.5, Oct: 16.7, Nov: 16.8, Dec: 16.9 Average annual CPI for 1942: 16.333333333333332 Year 1942 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1942 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 43,
      "chunk_index": 30,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_31_9c7b44e8",
    "text": "Inflation data for year 1943: Monthly CPI values: Jan: 16.9, Feb: 16.9, Mar: 17.2, Apr: 17.4, May: 17.5, Jun: 17.5, Jul: 17.4, Aug: 17.3, Sep: 17.4, Oct: 17.4, Nov: 17.4, Dec: 17.4 Average annual CPI for 1943: 17.308333333333337 Year 1943 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1943 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 44,
      "chunk_index": 31,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_32_0bab9b0b",
    "text": "Inflation data for year 1944: Monthly CPI values: Jan: 17.4, Feb: 17.4, Mar: 17.4, Apr: 17.5, May: 17.5, Jun: 17.6, Jul: 17.7, Aug: 17.7, Sep: 17.7, Oct: 17.7, Nov: 17.7, Dec: 17.8 Average annual CPI for 1944: 17.591666666666665 Year 1944 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1944 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 45,
      "chunk_index": 32,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_33_02d70036",
    "text": "Inflation data for year 1945: Monthly CPI values: Jan: 17.8, Feb: 17.8, Mar: 17.8, Apr: 17.8, May: 17.9, Jun: 18.1, Jul: 18.1, Aug: 18.1, Sep: 18.1, Oct: 18.1, Nov: 18.1, Dec: 18.2 Average annual CPI for 1945: 17.991666666666664 Year 1945 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1945 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 46,
      "chunk_index": 33,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_34_b1b30de1",
    "text": "Inflation data for year 1946: Monthly CPI values: Jan: 18.2, Feb: 18.1, Mar: 18.3, Apr: 18.4, May: 18.5, Jun: 18.7, Jul: 19.8, Aug: 20.2, Sep: 20.4, Oct: 20.8, Nov: 21.3, Dec: 21.5 Average annual CPI for 1946: 19.51666666666667 Year 1946 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1946 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 47,
      "chunk_index": 34,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_35_45d3813e",
    "text": "Inflation data for year 1947: Monthly CPI values: Jan: 21.5, Feb: 21.5, Mar: 21.9, Apr: 21.9, May: 21.9, Jun: 22, Jul: 22.2, Aug: 22.5, Sep: 23, Oct: 23, Nov: 23.1, Dec: 23.4 Average annual CPI for 1947: 22.325 Year 1947 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1947 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 48,
      "chunk_index": 35,
      "char_count": 371
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_36_1a997249",
    "text": "Inflation data for year 1948: Monthly CPI values: Jan: 23.7, Feb: 23.5, Mar: 23.4, Apr: 23.8, May: 23.9, Jun: 24.1, Jul: 24.4, Aug: 24.5, Sep: 24.5, Oct: 24.4, Nov: 24.2, Dec: 24.1 Average annual CPI for 1948: 24.041666666666668 Year 1948 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1948 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 49,
      "chunk_index": 36,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_37_f900dd7c",
    "text": "Inflation data for year 1949: Monthly CPI values: Jan: 24, Feb: 23.8, Mar: 23.8, Apr: 23.9, May: 23.8, Jun: 23.9, Jul: 23.7, Aug: 23.8, Sep: 23.9, Oct: 23.7, Nov: 23.8, Dec: 23.6 Average annual CPI for 1949: 23.808333333333334 Year 1949 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1949 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 50,
      "chunk_index": 37,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_38_87e7c8d7",
    "text": "Inflation data for year 1950: Monthly CPI values: Jan: 23.5, Feb: 23.5, Mar: 23.6, Apr: 23.6, May: 23.7, Jun: 23.8, Jul: 24.1, Aug: 24.3, Sep: 24.4, Oct: 24.6, Nov: 24.7, Dec: 25 Average annual CPI for 1950: 24.066666666666666 Year 1950 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1950 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 51,
      "chunk_index": 38,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_39_d2fd63be",
    "text": "Inflation data for year 1951: Monthly CPI values: Jan: 25.4, Feb: 25.7, Mar: 25.8, Apr: 25.8, May: 25.9, Jun: 25.9, Jul: 25.9, Aug: 25.9, Sep: 26.1, Oct: 26.2, Nov: 26.4, Dec: 26.5 Average annual CPI for 1951: 25.958333333333332 Year 1951 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1951 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 52,
      "chunk_index": 39,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_40_b25dd8c6",
    "text": "Inflation data for year 1952: Monthly CPI values: Jan: 26.5, Feb: 26.3, Mar: 26.3, Apr: 26.4, May: 26.4, Jun: 26.5, Jul: 26.7, Aug: 26.7, Sep: 26.7, Oct: 26.7, Nov: 26.7, Dec: 26.7 Average annual CPI for 1952: 26.549999999999997 Year 1952 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1952 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 53,
      "chunk_index": 40,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_41_1e6134ae",
    "text": "Inflation data for year 1953: Monthly CPI values: Jan: 26.6, Feb: 26.5, Mar: 26.6, Apr: 26.6, May: 26.7, Jun: 26.8, Jul: 26.8, Aug: 26.9, Sep: 26.9, Oct: 27, Nov: 26.9, Dec: 26.9 Average annual CPI for 1953: 26.766666666666666 Year 1953 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1953 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 54,
      "chunk_index": 41,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_42_58ddc0bb",
    "text": "Inflation data for year 1954: Monthly CPI values: Jan: 26.9, Feb: 26.9, Mar: 26.9, Apr: 26.8, May: 26.9, Jun: 26.9, Jul: 26.9, Aug: 26.9, Sep: 26.8, Oct: 26.8, Nov: 26.8, Dec: 26.7 Average annual CPI for 1954: 26.849999999999998 Year 1954 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1954 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 55,
      "chunk_index": 42,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_43_9dcc8eb7",
    "text": "Inflation data for year 1955: Monthly CPI values: Jan: 26.7, Feb: 26.7, Mar: 26.7, Apr: 26.7, May: 26.7, Jun: 26.7, Jul: 26.8, Aug: 26.8, Sep: 26.9, Oct: 26.9, Nov: 26.9, Dec: 26.8 Average annual CPI for 1955: 26.775000000000002 Year 1955 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1955 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 56,
      "chunk_index": 43,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_44_ee0ff9b6",
    "text": "Inflation data for year 1956: Monthly CPI values: Jan: 26.8, Feb: 26.8, Mar: 26.8, Apr: 26.9, May: 27, Jun: 27.2, Jul: 27.4, Aug: 27.3, Sep: 27.4, Oct: 27.5, Nov: 27.5, Dec: 27.6 Average annual CPI for 1956: 27.183333333333337 Year 1956 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1956 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 57,
      "chunk_index": 44,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_45_2cab9783",
    "text": "Inflation data for year 1957: Monthly CPI values: Jan: 27.6, Feb: 27.7, Mar: 27.8, Apr: 27.9, May: 28, Jun: 28.1, Jul: 28.3, Aug: 28.3, Sep: 28.3, Oct: 28.3, Nov: 28.4, Dec: 28.4 Average annual CPI for 1957: 28.091666666666665 Year 1957 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1957 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 58,
      "chunk_index": 45,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_46_1bd670af",
    "text": "Inflation data for year 1958: Monthly CPI values: Jan: 28.6, Feb: 28.6, Mar: 28.8, Apr: 28.9, May: 28.9, Jun: 28.9, Jul: 29, Aug: 28.9, Sep: 28.9, Oct: 28.9, Nov: 29, Dec: 28.9 Average annual CPI for 1958: 28.85833333333333 Year 1958 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1958 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 59,
      "chunk_index": 46,
      "char_count": 384
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_47_199ef7d7",
    "text": "Inflation data for year 1959: Monthly CPI values: Jan: 29, Feb: 28.9, Mar: 28.9, Apr: 29, May: 29, Jun: 29.1, Jul: 29.2, Aug: 29.2, Sep: 29.3, Oct: 29.4, Nov: 29.4, Dec: 29.4 Average annual CPI for 1959: 29.14999999999999 Year 1959 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1959 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 60,
      "chunk_index": 47,
      "char_count": 382
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_48_1084e275",
    "text": "Inflation data for year 1960: Monthly CPI values: Jan: 29.3, Feb: 29.4, Mar: 29.4, Apr: 29.5, May: 29.5, Jun: 29.6, Jul: 29.6, Aug: 29.6, Sep: 29.6, Oct: 29.8, Nov: 29.8, Dec: 29.8 Average annual CPI for 1960: 29.575000000000003 Year 1960 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1960 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 61,
      "chunk_index": 48,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_49_05ef87c4",
    "text": "Inflation data for year 1961: Monthly CPI values: Jan: 29.8, Feb: 29.8, Mar: 29.8, Apr: 29.8, May: 29.8, Jun: 29.8, Jul: 30, Aug: 29.9, Sep: 30, Oct: 30, Nov: 30, Dec: 30 Average annual CPI for 1961: 29.89166666666667 Year 1961 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1961 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 62,
      "chunk_index": 49,
      "char_count": 378
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_50_5affa59b",
    "text": "Inflation data for year 1962: Monthly CPI values: Jan: 30, Feb: 30.1, Mar: 30.1, Apr: 30.2, May: 30.2, Jun: 30.2, Jul: 30.3, Aug: 30.3, Sep: 30.4, Oct: 30.4, Nov: 30.4, Dec: 30.4 Average annual CPI for 1962: 30.249999999999996 Year 1962 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1962 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 63,
      "chunk_index": 50,
      "char_count": 387
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_51_c8865f84",
    "text": "Inflation data for year 1963: Monthly CPI values: Jan: 30.4, Feb: 30.4, Mar: 30.5, Apr: 30.5, May: 30.5, Jun: 30.6, Jul: 30.7, Aug: 30.7, Sep: 30.7, Oct: 30.8, Nov: 30.8, Dec: 30.9 Average annual CPI for 1963: 30.625 Year 1963 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1963 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 64,
      "chunk_index": 51,
      "char_count": 377
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_52_17cd85f2",
    "text": "Inflation data for year 1964: Monthly CPI values: Jan: 30.9, Feb: 30.9, Mar: 30.9, Apr: 30.9, May: 30.9, Jun: 31, Jul: 31.1, Aug: 31, Sep: 31.1, Oct: 31.1, Nov: 31.2, Dec: 31.2 Average annual CPI for 1964: 31.016666666666666 Year 1964 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1964 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 65,
      "chunk_index": 52,
      "char_count": 385
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_53_06b3d746",
    "text": "Inflation data for year 1965: Monthly CPI values: Jan: 31.2, Feb: 31.2, Mar: 31.3, Apr: 31.4, May: 31.4, Jun: 31.6, Jul: 31.6, Aug: 31.6, Sep: 31.6, Oct: 31.7, Nov: 31.7, Dec: 31.8 Average annual CPI for 1965: 31.50833333333333 Year 1965 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1965 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 66,
      "chunk_index": 53,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_54_87dc7715",
    "text": "Inflation data for year 1966: Monthly CPI values: Jan: 31.8, Feb: 32, Mar: 32.1, Apr: 32.3, May: 32.3, Jun: 32.4, Jul: 32.5, Aug: 32.7, Sep: 32.7, Oct: 32.9, Nov: 32.9, Dec: 32.9 Average annual CPI for 1966: 32.45833333333333 Year 1966 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1966 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 67,
      "chunk_index": 54,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_55_e8e58e39",
    "text": "Inflation data for year 1967: Monthly CPI values: Jan: 32.9, Feb: 32.9, Mar: 33, Apr: 33.1, May: 33.2, Jun: 33.3, Jul: 33.4, Aug: 33.5, Sep: 33.6, Oct: 33.7, Nov: 33.8, Dec: 33.9 Average annual CPI for 1967: 33.35833333333334 Year 1967 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1967 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 68,
      "chunk_index": 55,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_56_bd9205cf",
    "text": "Inflation data for year 1968: Monthly CPI values: Jan: 34.1, Feb: 34.2, Mar: 34.3, Apr: 34.4, May: 34.5, Jun: 34.7, Jul: 34.9, Aug: 35, Sep: 35.1, Oct: 35.3, Nov: 35.4, Dec: 35.5 Average annual CPI for 1968: 34.78333333333334 Year 1968 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1968 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 69,
      "chunk_index": 56,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_57_e3f0a314",
    "text": "Inflation data for year 1969: Monthly CPI values: Jan: 35.6, Feb: 35.8, Mar: 36.1, Apr: 36.3, May: 36.4, Jun: 36.6, Jul: 36.8, Aug: 37, Sep: 37.1, Oct: 37.3, Nov: 37.5, Dec: 37.7 Average annual CPI for 1969: 36.68333333333334 Year 1969 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1969 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 70,
      "chunk_index": 57,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_58_7abe4fc0",
    "text": "Inflation data for year 1970: Monthly CPI values: Jan: 37.8, Feb: 38, Mar: 38.2, Apr: 38.5, May: 38.6, Jun: 38.8, Jul: 39, Aug: 39, Sep: 39.2, Oct: 39.4, Nov: 39.6, Dec: 39.8 Average annual CPI for 1970: 38.824999999999996 Year 1970 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1970 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 71,
      "chunk_index": 58,
      "char_count": 383
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_59_d81ce575",
    "text": "Inflation data for year 1971: Monthly CPI values: Jan: 39.8, Feb: 39.9, Mar: 40, Apr: 40.1, May: 40.3, Jun: 40.6, Jul: 40.7, Aug: 40.8, Sep: 40.8, Oct: 40.9, Nov: 40.9, Dec: 41.1 Average annual CPI for 1971: 40.49166666666667 Year 1971 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1971 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 72,
      "chunk_index": 59,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_60_779df3c9",
    "text": "Inflation data for year 1972: Monthly CPI values: Jan: 41.1, Feb: 41.3, Mar: 41.4, Apr: 41.5, May: 41.6, Jun: 41.7, Jul: 41.9, Aug: 42, Sep: 42.1, Oct: 42.3, Nov: 42.4, Dec: 42.5 Average annual CPI for 1972: 41.81666666666667 Year 1972 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1972 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 73,
      "chunk_index": 60,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_61_e41a38a2",
    "text": "Inflation data for year 1973: Monthly CPI values: Jan: 42.6, Feb: 42.9, Mar: 43.3, Apr: 43.6, May: 43.9, Jun: 44.2, Jul: 44.3, Aug: 45.1, Sep: 45.2, Oct: 45.6, Nov: 45.9, Dec: 46.2 Average annual CPI for 1973: 44.400000000000006 Year 1973 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1973 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 74,
      "chunk_index": 61,
      "char_count": 389
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_62_3456b8bf",
    "text": "Inflation data for year 1974: Monthly CPI values: Jan: 46.6, Feb: 47.2, Mar: 47.8, Apr: 48, May: 48.6, Jun: 49, Jul: 49.4, Aug: 50, Sep: 50.6, Oct: 51.1, Nov: 51.5, Dec: 51.9 Average annual CPI for 1974: 49.30833333333334 Year 1974 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1974 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 75,
      "chunk_index": 62,
      "char_count": 382
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_63_af58d92b",
    "text": "Inflation data for year 1975: Monthly CPI values: Jan: 52.1, Feb: 52.5, Mar: 52.7, Apr: 52.9, May: 53.2, Jun: 53.6, Jul: 54.2, Aug: 54.3, Sep: 54.6, Oct: 54.9, Nov: 55.3, Dec: 55.5 Average annual CPI for 1975: 53.81666666666667 Year 1975 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1975 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 76,
      "chunk_index": 63,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_64_a1f1a8b4",
    "text": "Inflation data for year 1976: Monthly CPI values: Jan: 55.6, Feb: 55.8, Mar: 55.9, Apr: 56.1, May: 56.5, Jun: 56.8, Jul: 57.1, Aug: 57.4, Sep: 57.6, Oct: 57.9, Nov: 58, Dec: 58.2 Average annual CPI for 1976: 56.90833333333334 Year 1976 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1976 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 77,
      "chunk_index": 64,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_65_93b3f192",
    "text": "Inflation data for year 1977: Monthly CPI values: Jan: 58.5, Feb: 59.1, Mar: 59.5, Apr: 60, May: 60.3, Jun: 60.7, Jul: 61, Aug: 61.2, Sep: 61.4, Oct: 61.6, Nov: 61.9, Dec: 62.1 Average annual CPI for 1977: 60.60833333333333 Year 1977 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1977 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 78,
      "chunk_index": 65,
      "char_count": 384
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_66_26b0d581",
    "text": "Inflation data for year 1978: Monthly CPI values: Jan: 62.5, Feb: 62.9, Mar: 63.4, Apr: 63.9, May: 64.5, Jun: 65.2, Jul: 65.7, Aug: 66, Sep: 66.5, Oct: 67.1, Nov: 67.4, Dec: 67.7 Average annual CPI for 1978: 65.23333333333333 Year 1978 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1978 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 79,
      "chunk_index": 66,
      "char_count": 386
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_67_cc71fdcc",
    "text": "Inflation data for year 1979: Monthly CPI values: Jan: 68.3, Feb: 69.1, Mar: 69.8, Apr: 70.6, May: 71.5, Jun: 72.3, Jul: 73.1, Aug: 73.8, Sep: 74.6, Oct: 75.2, Nov: 75.9, Dec: 76.7 Average annual CPI for 1979: 72.575 Year 1979 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1979 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 80,
      "chunk_index": 67,
      "char_count": 377
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_68_df9c6693",
    "text": "Inflation data for year 1980: Monthly CPI values: Jan: 77.8, Feb: 78.9, Mar: 80.1, Apr: 81, May: 81.8, Jun: 82.7, Jul: 82.7, Aug: 83.3, Sep: 84, Oct: 84.8, Nov: 85.5, Dec: 86.3 Average annual CPI for 1980: 82.40833333333332 Year 1980 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1980 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 81,
      "chunk_index": 68,
      "char_count": 384
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_69_6006d697",
    "text": "Inflation data for year 1981: Monthly CPI values: Jan: 87, Feb: 87.9, Mar: 88.5, Apr: 89.1, May: 89.8, Jun: 90.6, Jul: 91.6, Aug: 92.3, Sep: 93.2, Oct: 93.4, Nov: 93.7, Dec: 94 Average annual CPI for 1981: 90.925 Year 1981 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1981 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 82,
      "chunk_index": 69,
      "char_count": 373
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_70_47e2b50d",
    "text": "Inflation data for year 1982: Monthly CPI values: Jan: 94.3, Feb: 94.6, Mar: 94.5, Apr: 94.9, May: 95.8, Jun: 97, Jul: 97.5, Aug: 97.7, Sep: 97.9, Oct: 98.2, Nov: 98, Dec: 97.6 Average annual CPI for 1982: 96.5 Year 1982 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1982 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 83,
      "chunk_index": 70,
      "char_count": 371
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_71_34977f2e",
    "text": "Inflation data for year 1983: Monthly CPI values: Jan: 97.8, Feb: 97.9, Mar: 97.9, Apr: 98.6, May: 99.2, Jun: 99.5, Jul: 99.9, Aug: 100.2, Sep: 100.7, Oct: 101, Nov: 101.2, Dec: 101.3 Average annual CPI for 1983: 99.60000000000001 Year 1983 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1983 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 84,
      "chunk_index": 71,
      "char_count": 391
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_72_38675722",
    "text": "Inflation data for year 1984: Monthly CPI values: Jan: 101.9, Feb: 102.4, Mar: 102.6, Apr: 103.1, May: 103.4, Jun: 103.7, Jul: 104.1, Aug: 104.5, Sep: 105, Oct: 105.3, Nov: 105.3, Dec: 105.3 Average annual CPI for 1984: 103.88333333333333 Year 1984 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1984 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 85,
      "chunk_index": 72,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_73_3cd4fa55",
    "text": "Inflation data for year 1985: Monthly CPI values: Jan: 105.5, Feb: 106, Mar: 106.4, Apr: 106.9, May: 107.3, Jun: 107.6, Jul: 107.8, Aug: 108, Sep: 108.3, Oct: 108.7, Nov: 109, Dec: 109.3 Average annual CPI for 1985: 107.56666666666665 Year 1985 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1985 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 86,
      "chunk_index": 73,
      "char_count": 395
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_74_8274bec0",
    "text": "Inflation data for year 1986: Monthly CPI values: Jan: 109.6, Feb: 109.3, Mar: 108.8, Apr: 108.6, May: 108.9, Jun: 109.5, Jul: 109.5, Aug: 109.7, Sep: 110.2, Oct: 110.3, Nov: 110.4, Dec: 110.5 Average annual CPI for 1986: 109.60833333333335 Year 1986 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1986 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 87,
      "chunk_index": 74,
      "char_count": 401
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_75_5e751c20",
    "text": "Inflation data for year 1987: Monthly CPI values: Jan: 111.2, Feb: 111.6, Mar: 112.1, Apr: 112.7, May: 113.1, Jun: 113.5, Jul: 113.8, Aug: 114.4, Sep: 115, Oct: 115.3, Nov: 115.4, Dec: 115.4 Average annual CPI for 1987: 113.625 Year 1987 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1987 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 88,
      "chunk_index": 75,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_76_d7d6da69",
    "text": "Inflation data for year 1988: Monthly CPI values: Jan: 115.7, Feb: 116, Mar: 116.5, Apr: 117.1, May: 117.5, Jun: 118, Jul: 118.5, Aug: 119, Sep: 119.8, Oct: 120.2, Nov: 120.3, Dec: 120.5 Average annual CPI for 1988: 118.25833333333333 Year 1988 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1988 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 89,
      "chunk_index": 76,
      "char_count": 395
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_77_efb96be0",
    "text": "Inflation data for year 1989: Monthly CPI values: Jan: 121.1, Feb: 121.6, Mar: 122.3, Apr: 123.1, May: 123.8, Jun: 124.1, Jul: 124.4, Aug: 124.6, Sep: 125, Oct: 125.6, Nov: 125.9, Dec: 126.1 Average annual CPI for 1989: 123.96666666666665 Year 1989 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1989 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 90,
      "chunk_index": 77,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_78_0b9d1f5e",
    "text": "Inflation data for year 1990: Monthly CPI values: Jan: 127.4, Feb: 128, Mar: 128.7, Apr: 128.9, May: 129.2, Jun: 129.9, Jul: 130.4, Aug: 131.6, Sep: 132.7, Oct: 133.5, Nov: 133.8, Dec: 133.8 Average annual CPI for 1990: 130.65833333333333 Year 1990 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1990 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 91,
      "chunk_index": 78,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_79_e59d36fe",
    "text": "Inflation data for year 1991: Monthly CPI values: Jan: 134.6, Feb: 134.8, Mar: 135, Apr: 135.2, May: 135.6, Jun: 136, Jul: 136.2, Aug: 136.6, Sep: 137.2, Oct: 137.4, Nov: 137.8, Dec: 137.9 Average annual CPI for 1991: 136.19166666666666 Year 1991 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1991 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 92,
      "chunk_index": 79,
      "char_count": 397
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_80_1279b615",
    "text": "Inflation data for year 1992: Monthly CPI values: Jan: 138.1, Feb: 138.6, Mar: 139.3, Apr: 139.5, May: 139.7, Jun: 140.2, Jul: 140.5, Aug: 140.9, Sep: 141.3, Oct: 141.8, Nov: 142, Dec: 141.9 Average annual CPI for 1992: 140.3166666666667 Year 1992 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1992 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 93,
      "chunk_index": 80,
      "char_count": 398
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_81_f89b1af2",
    "text": "Inflation data for year 1993: Monthly CPI values: Jan: 142.6, Feb: 143.1, Mar: 143.6, Apr: 144, May: 144.2, Jun: 144.4, Jul: 144.4, Aug: 144.8, Sep: 145.1, Oct: 145.7, Nov: 145.8, Dec: 145.8 Average annual CPI for 1993: 144.45833333333331 Year 1993 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1993 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 94,
      "chunk_index": 81,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_82_1a49efcd",
    "text": "Inflation data for year 1994: Monthly CPI values: Jan: 146.2, Feb: 146.7, Mar: 147.2, Apr: 147.4, May: 147.5, Jun: 148, Jul: 148.4, Aug: 149, Sep: 149.4, Oct: 149.5, Nov: 149.7, Dec: 149.7 Average annual CPI for 1994: 148.22500000000002 Year 1994 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1994 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 95,
      "chunk_index": 82,
      "char_count": 397
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_83_430373db",
    "text": "Inflation data for year 1995: Monthly CPI values: Jan: 150.3, Feb: 150.9, Mar: 151.4, Apr: 151.9, May: 152.2, Jun: 152.5, Jul: 152.5, Aug: 152.9, Sep: 153.2, Oct: 153.7, Nov: 153.6, Dec: 153.5 Average annual CPI for 1995: 152.38333333333335 Year 1995 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1995 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 96,
      "chunk_index": 83,
      "char_count": 401
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_84_c6357133",
    "text": "Inflation data for year 1996: Monthly CPI values: Jan: 154.4, Feb: 154.9, Mar: 155.7, Apr: 156.3, May: 156.6, Jun: 156.7, Jul: 157, Aug: 157.3, Sep: 157.8, Oct: 158.3, Nov: 158.6, Dec: 158.6 Average annual CPI for 1996: 156.84999999999997 Year 1996 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1996 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 97,
      "chunk_index": 84,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_85_ce12008e",
    "text": "Inflation data for year 1997: Monthly CPI values: Jan: 159.1, Feb: 159.6, Mar: 160, Apr: 160.2, May: 160.1, Jun: 160.3, Jul: 160.5, Aug: 160.8, Sep: 161.2, Oct: 161.6, Nov: 161.5, Dec: 161.3 Average annual CPI for 1997: 160.51666666666665 Year 1997 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1997 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 98,
      "chunk_index": 85,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_86_cf00bc29",
    "text": "Inflation data for year 1998: Monthly CPI values: Jan: 161.6, Feb: 161.9, Mar: 162.2, Apr: 162.5, May: 162.8, Jun: 163, Jul: 163.2, Aug: 163.4, Sep: 163.6, Oct: 164, Nov: 164, Dec: 163.9 Average annual CPI for 1998: 163.00833333333335 Year 1998 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1998 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 99,
      "chunk_index": 86,
      "char_count": 395
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_87_a7a07a37",
    "text": "Inflation data for year 1999: Monthly CPI values: Jan: 164.3, Feb: 164.5, Mar: 165, Apr: 166.2, May: 166.2, Jun: 166.2, Jul: 166.7, Aug: 167.1, Sep: 167.9, Oct: 168.2, Nov: 168.3, Dec: 168.3 Average annual CPI for 1999: 166.57500000000002 Year 1999 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 1999 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 100,
      "chunk_index": 87,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_88_cf0b1537",
    "text": "Inflation data for year 2000: Monthly CPI values: Jan: 168.8, Feb: 169.8, Mar: 171.2, Apr: 171.3, May: 171.5, Jun: 172.4, Jul: 172.8, Aug: 172.8, Sep: 173.7, Oct: 174, Nov: 174.1, Dec: 174 Average annual CPI for 2000: 172.19999999999996 Year 2000 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2000 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 101,
      "chunk_index": 88,
      "char_count": 397
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_89_a10e1c40",
    "text": "Inflation data for year 2001: Monthly CPI values: Jan: 175.1, Feb: 175.8, Mar: 176.2, Apr: 176.9, May: 177.7, Jun: 178, Jul: 177.5, Aug: 177.5, Sep: 178.3, Oct: 177.7, Nov: 177.4, Dec: 176.7 Average annual CPI for 2001: 177.06666666666663 Year 2001 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2001 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 102,
      "chunk_index": 89,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_90_67e0265a",
    "text": "Inflation data for year 2002: Monthly CPI values: Jan: 177.1, Feb: 177.8, Mar: 178.8, Apr: 179.8, May: 179.8, Jun: 179.9, Jul: 180.1, Aug: 180.7, Sep: 181, Oct: 181.3, Nov: 181.3, Dec: 180.9 Average annual CPI for 2002: 179.875 Year 2002 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2002 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 103,
      "chunk_index": 90,
      "char_count": 388
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_91_ddf59b4b",
    "text": "Inflation data for year 2003: Monthly CPI values: Jan: 181.7, Feb: 183.1, Mar: 184.2, Apr: 183.8, May: 183.5, Jun: 183.7, Jul: 183.9, Aug: 184.6, Sep: 185.2, Oct: 185, Nov: 184.5, Dec: 184.3 Average annual CPI for 2003: 183.95833333333334 Year 2003 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2003 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 104,
      "chunk_index": 91,
      "char_count": 399
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_92_2dc3db93",
    "text": "Inflation data for year 2004: Monthly CPI values: Jan: 185.2, Feb: 186.2, Mar: 187.4, Apr: 188, May: 189.1, Jun: 189.7, Jul: 189.4, Aug: 189.5, Sep: 189.9, Oct: 190.9, Nov: 191, Dec: 190.3 Average annual CPI for 2004: 188.88333333333335 Year 2004 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2004 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 105,
      "chunk_index": 92,
      "char_count": 397
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_93_ef4b2325",
    "text": "Inflation data for year 2005: Monthly CPI values: Jan: 190.7, Feb: 191.8, Mar: 193.3, Apr: 194.6, May: 194.4, Jun: 194.5, Jul: 195.4, Aug: 196.4, Sep: 198.8, Oct: 199.2, Nov: 197.6, Dec: 196.8 Average annual CPI for 2005: 195.2916666666667 Year 2005 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2005 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 106,
      "chunk_index": 93,
      "char_count": 400
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_94_4eed5e1b",
    "text": "Inflation data for year 2006: Monthly CPI values: Jan: 198.3, Feb: 198.7, Mar: 199.8, Apr: 201.5, May: 202.5, Jun: 202.9, Jul: 203.5, Aug: 203.9, Sep: 202.9, Oct: 201.8, Nov: 201.5, Dec: 201.8 Average annual CPI for 2006: 201.5916666666667 Year 2006 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2006 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 107,
      "chunk_index": 94,
      "char_count": 400
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_95_c08baaff",
    "text": "Inflation data for year 2007: Monthly CPI values: Jan: 202.416, Feb: 203.499, Mar: 205.352, Apr: 206.686, May: 207.949, Jun: 208.352, Jul: 208.299, Aug: 207.917, Sep: 208.49, Oct: 208.936, Nov: 210.177, Dec: 210.036 Average annual CPI for 2007: 207.3424166666667 Year 2007 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2007 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 108,
      "chunk_index": 95,
      "char_count": 423
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_96_b67abb1e",
    "text": "Inflation data for year 2008: Monthly CPI values: Jan: 211.08, Feb: 211.693, Mar: 213.528, Apr: 214.823, May: 216.632, Jun: 218.815, Jul: 219.964, Aug: 219.086, Sep: 218.783, Oct: 216.573, Nov: 212.425, Dec: 210.228 Average annual CPI for 2008: 215.3025 Year 2008 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2008 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 109,
      "chunk_index": 96,
      "char_count": 414
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_97_52f3c4ed",
    "text": "Inflation data for year 2009: Monthly CPI values: Jan: 211.143, Feb: 212.193, Mar: 212.709, Apr: 213.24, May: 213.856, Jun: 215.693, Jul: 215.351, Aug: 215.834, Sep: 215.969, Oct: 216.177, Nov: 216.33, Dec: 215.949 Average annual CPI for 2009: 214.537 Year 2009 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2009 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 110,
      "chunk_index": 97,
      "char_count": 412
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_98_525ce9db",
    "text": "Inflation data for year 2010: Monthly CPI values: Jan: 216.687, Feb: 216.741, Mar: 217.631, Apr: 218.009, May: 218.178, Jun: 217.965, Jul: 218.011, Aug: 218.312, Sep: 218.439, Oct: 218.711, Nov: 218.803, Dec: 219.179 Average annual CPI for 2010: 218.05550000000002 Year 2010 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2010 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 111,
      "chunk_index": 98,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_99_3769fa49",
    "text": "Inflation data for year 2011: Monthly CPI values: Jan: 220.223, Feb: 221.309, Mar: 223.467, Apr: 224.906, May: 225.964, Jun: 225.722, Jul: 225.922, Aug: 226.545, Sep: 226.889, Oct: 226.421, Nov: 226.23, Dec: 225.672 Average annual CPI for 2011: 224.93916666666667 Year 2011 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2011 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 112,
      "chunk_index": 99,
      "char_count": 424
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_100_18818a75",
    "text": "Inflation data for year 2012: Monthly CPI values: Jan: 226.665, Feb: 227.663, Mar: 229.392, Apr: 230.085, May: 229.815, Jun: 229.478, Jul: 229.104, Aug: 230.379, Sep: 231.407, Oct: 231.317, Nov: 230.221, Dec: 229.601 Average annual CPI for 2012: 229.5939166666667 Year 2012 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2012 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 113,
      "chunk_index": 100,
      "char_count": 424
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_101_2ec71db6",
    "text": "Inflation data for year 2013: Monthly CPI values: Jan: 230.28, Feb: 232.166, Mar: 232.773, Apr: 232.531, May: 232.945, Jun: 233.504, Jul: 233.596, Aug: 233.877, Sep: 234.149, Oct: 233.546, Nov: 233.069, Dec: 233.049 Average annual CPI for 2013: 232.95708333333332 Year 2013 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2013 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 114,
      "chunk_index": 101,
      "char_count": 424
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_102_ffff68dc",
    "text": "Inflation data for year 2014: Monthly CPI values: Jan: 233.916, Feb: 234.781, Mar: 236.293, Apr: 237.072, May: 237.9, Jun: 238.343, Jul: 238.25, Aug: 237.852, Sep: 238.031, Oct: 237.433, Nov: 236.151, Dec: 234.812 Average annual CPI for 2014: 236.73616666666666 Year 2014 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2014 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 115,
      "chunk_index": 102,
      "char_count": 422
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_103_2cec94b7",
    "text": "Inflation data for year 2015: Monthly CPI values: Jan: 233.707, Feb: 234.722, Mar: 236.119, Apr: 236.599, May: 237.805, Jun: 238.638, Jul: 238.654, Aug: 238.316, Sep: 237.945, Oct: 237.838, Nov: 237.336, Dec: 236.525 Average annual CPI for 2015: 237.01700000000002 Year 2015 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2015 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 116,
      "chunk_index": 103,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_104_c6889f49",
    "text": "Inflation data for year 2016: Monthly CPI values: Jan: 236.916, Feb: 237.111, Mar: 238.132, Apr: 239.261, May: 240.229, Jun: 241.018, Jul: 240.628, Aug: 240.849, Sep: 241.428, Oct: 241.729, Nov: 241.353, Dec: 241.432 Average annual CPI for 2016: 240.00716666666662 Year 2016 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2016 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 117,
      "chunk_index": 104,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_105_4a0590db",
    "text": "Inflation data for year 2017: Monthly CPI values: Jan: 242.839, Feb: 243.603, Mar: 243.801, Apr: 244.524, May: 244.733, Jun: 244.955, Jul: 244.786, Aug: 245.519, Sep: 246.819, Oct: 246.663, Nov: 246.669, Dec: 246.524 Average annual CPI for 2017: 245.11958333333334 Year 2017 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2017 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 118,
      "chunk_index": 105,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_106_4c6ee84f",
    "text": "Inflation data for year 2018: Monthly CPI values: Jan: 247.867, Feb: 248.991, Mar: 249.554, Apr: 250.546, May: 251.588, Jun: 251.989, Jul: 252.006, Aug: 252.146, Sep: 252.439, Oct: 252.885, Nov: 252.038, Dec: 251.233 Average annual CPI for 2018: 251.10683333333338 Year 2018 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2018 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 119,
      "chunk_index": 106,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_107_e42351fa",
    "text": "Inflation data for year 2019: Monthly CPI values: Jan: 251.712, Feb: 252.776, Mar: 254.202, Apr: 255.548, May: 256.092, Jun: 256.143, Jul: 256.571, Aug: 256.558, Sep: 256.759, Oct: 257.346, Nov: 257.208, Dec: 256.974 Average annual CPI for 2019: 255.65741666666668 Year 2019 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2019 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 120,
      "chunk_index": 107,
      "char_count": 425
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_108_4a6086c6",
    "text": "Inflation data for year 2020: Monthly CPI values: Jan: 257.971, Feb: 258.678, Mar: 258.115, Apr: 256.389, May: 256.394, Jun: 257.797, Jul: 259.101, Aug: 259.918, Sep: 260.28, Oct: 260.388, Nov: 260.229, Dec: 260.474 Average annual CPI for 2020: 258.8111666666667 Year 2020 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2020 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 121,
      "chunk_index": 108,
      "char_count": 423
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_109_28de658f",
    "text": "Inflation data for year 2021: Monthly CPI values: Jan: 261.582, Feb: 263.014, Mar: 264.877, Apr: 267.054, May: 269.195, Jun: 271.696, Jul: 273.003, Aug: 273.567, Sep: 274.31, Oct: 276.589, Nov: 277.948, Dec: 278.802 Average annual CPI for 2021: 270.96975000000003 Year 2021 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2021 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 122,
      "chunk_index": 109,
      "char_count": 424
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_110_242da38e",
    "text": "Inflation data for year 2022: Monthly CPI values: Jan: 281.148, Feb: 283.716, Mar: 287.504, Apr: 289.109, May: 292.296 Average annual CPI for 2022: 286.75460000000004 Year 2022 inflation data can be used to calculate inflation-adjusted values. To adjust a value from 2022 to another year, use the CPI ratio between those years.",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": 123,
      "chunk_index": 110,
      "char_count": 327
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_111_8b279225",
    "text": "Inflation data summary for the 1910s: Years covered: 1913 to 1919 Year-by-year average CPI: 1913: 9.883333333333335 1914: 10.016666666666666 1915: 10.108333333333333 1916: 10.883333333333333 1917: 12.824999999999998 1918: 15.041666666666666 1919: 17.333333333333332",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1910",
      "chunk_index": 111,
      "char_count": 265
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_112_761f6448",
    "text": "Inflation data summary for the 1920s: Years covered: 1920 to 1929 Year-by-year average CPI: 1920: 20.04166666666667 1921: 17.850000000000005 1922: 16.75 1923: 17.050000000000004 1924: 17.124999999999996 1925: 17.541666666666664 1926: 17.7 1927: 17.358333333333338 1928: 17.15833333333333 1929: 17.158333333333335",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1920",
      "chunk_index": 112,
      "char_count": 312
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_113_92b65ba2",
    "text": "Inflation data summary for the 1930s: Years covered: 1930 to 1939 Year-by-year average CPI: 1930: 16.7 1931: 15.20833333333333 1932: 13.641666666666666 1933: 12.933333333333332 1934: 13.383333333333333 1935: 13.725000000000001 1936: 13.866666666666667 1937: 14.383333333333335 1938: 14.091666666666663 1939: 13.908333333333331",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1930",
      "chunk_index": 113,
      "char_count": 326
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_114_7f8eb7fe",
    "text": "Inflation data summary for the 1940s: Years covered: 1940 to 1949 Year-by-year average CPI: 1940: 14.008333333333333 1941: 14.725000000000003 1942: 16.333333333333332 1943: 17.308333333333337 1944: 17.591666666666665 1945: 17.991666666666664 1946: 19.51666666666667 1947: 22.325 1948: 24.041666666666668 1949: 23.808333333333334",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1940",
      "chunk_index": 114,
      "char_count": 328
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_115_02a27b67",
    "text": "Inflation data summary for the 1950s: Years covered: 1950 to 1959 Year-by-year average CPI: 1950: 24.066666666666666 1951: 25.958333333333332 1952: 26.549999999999997 1953: 26.766666666666666 1954: 26.849999999999998 1955: 26.775000000000002 1956: 27.183333333333337 1957: 28.091666666666665 1958: 28.85833333333333 1959: 29.14999999999999",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1950",
      "chunk_index": 115,
      "char_count": 339
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_116_270e22ea",
    "text": "Inflation data summary for the 1960s: Years covered: 1960 to 1969 Year-by-year average CPI: 1960: 29.575000000000003 1961: 29.89166666666667 1962: 30.249999999999996 1963: 30.625 1964: 31.016666666666666 1965: 31.50833333333333 1966: 32.45833333333333 1967: 33.35833333333334 1968: 34.78333333333334 1969: 36.68333333333334",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1960",
      "chunk_index": 116,
      "char_count": 323
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_117_912ff59d",
    "text": "Inflation data summary for the 1970s: Years covered: 1970 to 1979 Year-by-year average CPI: 1970: 38.824999999999996 1971: 40.49166666666667 1972: 41.81666666666667 1973: 44.400000000000006 1974: 49.30833333333334 1975: 53.81666666666667 1976: 56.90833333333334 1977: 60.60833333333333 1978: 65.23333333333333 1979: 72.575",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1970",
      "chunk_index": 117,
      "char_count": 322
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_118_df8bece6",
    "text": "Inflation data summary for the 1980s: Years covered: 1980 to 1989 Year-by-year average CPI: 1980: 82.40833333333332 1981: 90.925 1982: 96.5 1983: 99.60000000000001 1984: 103.88333333333333 1985: 107.56666666666665 1986: 109.60833333333335 1987: 113.625 1988: 118.25833333333333 1989: 123.96666666666665",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1980",
      "chunk_index": 118,
      "char_count": 302
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_119_86df18b3",
    "text": "Inflation data summary for the 1990s: Years covered: 1990 to 1999 Year-by-year average CPI: 1990: 130.65833333333333 1991: 136.19166666666666 1992: 140.3166666666667 1993: 144.45833333333331 1994: 148.22500000000002 1995: 152.38333333333335 1996: 156.84999999999997 1997: 160.51666666666665 1998: 163.00833333333335 1999: 166.57500000000002",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_1990",
      "chunk_index": 119,
      "char_count": 340
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_120_5cb2ac38",
    "text": "Inflation data summary for the 2000s: Years covered: 2000 to 2009 Year-by-year average CPI: 2000: 172.19999999999996 2001: 177.06666666666663 2002: 179.875 2003: 183.95833333333334 2004: 188.88333333333335 2005: 195.2916666666667 2006: 201.5916666666667 2007: 207.3424166666667 2008: 215.3025 2009: 214.537",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_2000",
      "chunk_index": 120,
      "char_count": 306
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_121_ac84714e",
    "text": "Inflation data summary for the 2010s: Years covered: 2010 to 2019 Year-by-year average CPI: 2010: 218.05550000000002 2011: 224.93916666666667 2012: 229.5939166666667 2013: 232.95708333333332 2014: 236.73616666666666 2015: 237.01700000000002 2016: 240.00716666666662 2017: 245.11958333333334 2018: 251.10683333333338 2019: 255.65741666666668",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_2010",
      "chunk_index": 121,
      "char_count": 340
    }
  },
  {
    "chunk_id": "Inflation_Calculator_xlsx_122_c5cc6d2a",
    "text": "Inflation data summary for the 2020s: Years covered: 2020 to 2022 Year-by-year average CPI: 2020: 258.8111666666667 2021: 270.96975000000003 2022: 286.75460000000004",
    "metadata": {
      "source": "Inflation Calculator.xlsx",
      "doc_name": "inflation",
      "doc_type": "excel",
      "sheet": "INFLATION.CALCULATOR",
      "row": "decade_2020",
      "chunk_index": 122,
      "char_count": 165
    }
  }
]